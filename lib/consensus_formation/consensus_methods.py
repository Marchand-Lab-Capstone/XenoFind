"""
consensus_methods.py 
J. Sumabat, N. Lai, S. Peck, Y. Huang, 3/14/2024

consensus_methods.py contains the primary methods involved in forming a first-pass
consensus on a DNA sequence with potential XNA bases. 

basecall_command() - Generates a command to run basecalling on pod5 reads using dorado
map_to_reference() - Generates a command to map a basecalled sequence to a reference fasta
vsearch_command() - Generates a command to run cluster vsearch on a fasta file
medaka_consensus_command() - Generates a command to run medaka's consensus on a fasta file.
read_trim() - Generates a list of reads that are trimmed within 95% of the reference.
sort_fasta() - Generates a list of reads, sorted by length.
filter_cluster_size() - Generates a list of reads filtered to a cluster size -- NONFUNCTIONAL
write_to_fasta() - Is able to take a list of reads generated by the previous three methods, and write them to a fasta file. 
first_consensus() - Runs all of the steps involved with generating a consensus.

Danger Zone: these were added specifically for interplay with xf_low_qual, they could be considered last minute additions, and out of scope.
extract_n_indexes() - Pulls the first and last index of the unknown bases from a fasta reference file.
rename_consensus_headers() - renames the headers of a medaka consensus file to have the first and last
                             indexes of the unknown bases.
                             

"""


from Bio import SeqIO
import os
import pysam
import raw_read_merger as rrm
import setup_methods as setup


def basecall_command(basecaller_path, pod5_path, out_path, out_name):
    """
    Basecall_command generates a command to run the basecaller dorado.
    
    Parameters:
    basecaller_path: path to the basecaller, as str
    pod5_path: path to the pod5 file to be basecalled, as str
    out_path: path to the output directory, as str
    out_name: name the basecalled fq file will be given, as str
    
    Returns:
    a command string.
    """
    # Currently only supports Dorado
    cmd = "{} basecaller hac --no-trim --emit-fastq {} > {}{}.fq".format(basecaller_path, pod5_path, out_path, out_name)
    print('[Basecalling]: Command Generated: "{}"'.format(cmd))
    return cmd


def map_to_reference(mapper_path, reference_path, basecall_path, out_path, out_name):
    """
    map_to_reference generates a command to use minimap2 to align the basecall with
    a reference sequence.
    
    Parameters:
    mapper_path: path to minimap2 as str
    reference_path: path to reference fasta as str
    basecall_path: path to basecalled .fq file, as str
    out_path: path to the output directory, as str
    out_name: name the output sam file will be given, as str
    """
    # Currently only supports minimap2
    cmd = "{} -ax map-ont --score-N 0 --MD --min-dp-score 10 {} {} > {}{}.sam".format(mapper_path, reference_path, basecall_path, out_path, out_name) #probably doesn't need minimap2 as a separate path

    print('[Mapping]: Command Generated: "{}"'.format(cmd))
    return cmd

def filter_primary_alignments(sam_path):
    """
    strand_decouple takes in a fasta file generated from minimap2 and separates 
    it on forward and reverse strand reads using samtools. This function will 
    also generate a primary 
    
    Parameters: 
    sam_path: path to the sam file as a string,
    
    Returns: 
    primary only sam file 
    
    NOTE: NEED TO EDIT THIS FUNCTION INTO TWO FUNCTIONS, GENERATE PRIMARY SAM FILE PATH STRING AND RUN IT IN THE FIRST PASS FUNCTION. SECOND FUNCTION TO GENERATE THE FORWARD AND REVERSE STRINGS
    """
    
    # doing in function primary only alignment
    
    # Generating the index file for the sam file to index using pysam 
    cmd = 'samtools index ' + sam_path #creates index file for BAM
    os.system(cmd)
    
    # Extracting the sam path directory so the filtered version is written in the 
    # ame directory
    directory = os.path.dirname(sam_path)
    output_sam = os.path.join(directory, 'primary_alignments.sam')


    # Using pysam to only keep primary aligned reads 
    with pysam.AlignmentFile(input_sam, "r") as infile, \
         pysam.AlignmentFile(output_sam, "w", header=infile.header) as outfile:

        for read in infile:
            if not read.is_secondary and not read.is_supplementary and not read.is_unmapped:
                outfile.write(read)
                
        print('XenoFind [STATUS] - Primary Only SAM file generated, now generating ')
        return output_sam
        
        
#def strand_decouple(primary_sam_path, forward_out_path, reverse_out_path): #hae outpaths be GENERATED
def strand_decouple(primary_sam_path):
    """
    strand_decouple takes in a fasta file generated from minimap2 and separates 
    it by forward and reverse strand reads using samtools. 
    
    Parameters: 
    sam_path: path to the sam file as a string,
    
    Returns: 
    forward and reverse strand  sam files as a string. ALso generates these files
    in a directory. 
    
    NOTE: NEED TO EDIT THIS FUNCTION INTO TWO FUNCTIONS, GENERATE PRIMARY SAM FILE PATH STRING AND RUN IT IN THE FIRST PASS FUNCTION. SECOND FUNCTION TO GENERATE THE FORWARD AND REVERSE STRINGS
    """
    forward_out_path = os.path.join(os.path.dirname(primary_sam_path), 'forward.sam')
    reverse_out_path = os.path.join(os.path.dirname(primary_sam_path), 'reverse.sam')
    #This section actually generates the forward and reverse only reads 
    # Open the input SAM file for reading
    with pysam.AlignmentFile(output_sam, "r") as infile:

        # Open two files for writing: one for forward strand reads, another for reverse strand reads
        with pysam.AlignmentFile(forward_out_path, "w", header=infile.header) as outfile_forward, \
             pysam.AlignmentFile(reverse_out_path, "w", header=infile.header) as outfile_reverse:

            # Iterate through reads in the input file
            for read in infile:
                # Check if the read is mapped to the reverse strand
                if read.is_reverse:
                    # Write the read to the reverse strand reads file
                    outfile_reverse.write(read)
                else:
                    # Otherwise, write the read to the forward strand reads file
                    outfile_forward.write(read)
        return forward_out_path, reverse_out_path #maybe dont need to return these but will leave this here for now 

def read_trim(sam_path):
    """
    read_trim takes in a samfile and returns a list of the reads
    in that sam file, with the query name and alignment sequence,
    so long as they are mapped and >=95% of the reference length.
    
    Parameters:
    sam_path: path to the sam file in question as a string.
    
    Returns:
    list of queries and alignments as a string.
    """
    # Sam path is the basecalled to reference
    output_list = []
    
    # open the alignment sam using pysam, open the fasta path as a fasta file
    with pysam.AlignmentFile(sam_path, 'r') as samfile:
 
        # Set up variables for #unmapped reads & #outside length
        num_unmapped = 0
        num_outside = 0
        
        # for each read in the samfile,
        for read in samfile.fetch():

            # Check that the read is mapped
            if (not read.is_unmapped):

                # Get the reference sequence length and the alignment length
                reference_length = samfile.get_reference_length(read.reference_name)
                aligned_length = read.reference_length

                # if the aligned length is greater or equal to 95% of the reference length,
                if aligned_length >= reference_length * .95:
                    
                    # append it to the output.
                    output_list.append(f">{read.query_name}\n{read.query_alignment_sequence}")
                else:
                    num_outside += 1

            else:
                num_unmapped += 1
    print("[Trimming]: {} unmapped reads, {} short reads removed.".format(num_unmapped, num_outside))
    
    # return the output list
    return output_list


# How to validate the read_trim worked?

def sort_fasta(fasta_path):
    """
    sort_fasta takes in a fasta filepath and sorts it by length.
    
    Parameters:
    fasta_path: a path to a trimmed fasta file.

    
    Returns:
    a list of the records, sorted by length.
    """
    # get SeqRecord iterator as list by parsing the fasta file and passing fasta format.
    records = list(SeqIO.parse(fasta_path, "fasta"))
    
    # Sort the records by length using in-built sorting by sequence length.
    sorted_records = sorted(records, key=lambda x: len(x.seq))
    
    # create an empty list to hold the records to be output.
    output_records = []

    for record in sorted_records:
        output_records.append(">{}\n{}".format(record.id, record.seq))

    print("[Sorting]: {} records resorted.".format(len(output_records)))
    
    # return the sorted_records list
    return output_records


def vsearch_command(vsearch_path, fasta_path, out_path, out_name, sim_id):
    """
    vsearch_command takes in a fasta path, a similarity value, an output_path, and an output filename,
    and generates a command to perform clustering/ rough consensus formation on the fasta file.
    
    Parameters:
    vsearch_path: path to vsearch as a string.
    fasta_path: path to the fasta file in question, as a string.
    out_path: path to output the final consensus fasta.
    out_name: filename for the fasta file. 
    sim_id: float value representing how similar the centroids can be (between 0 and 1)
    
    Returns: 
    command to perform vsearch with given directories as a string.
    """
    
    # Generate the command.
    cmd = "{} --cluster_fast {} --id {} --clusterout_sort --consout {}{}.fasta".format(vsearch_path, fasta_path, sim_id, out_path, out_name)

    print('[Vsearching]: Command Generated: "{}"'.format(cmd))
    return cmd
            
    
def filter_cluster_size(fasta_path, threshold=1):
    """
    filter_cluster_size takes in a fasta filepath and a threshold,
    and removes all clusters that are not within that size threshold.
    
    Parameters:
    fasta_path: path to clustered/consensus fasta file, as string.
    threshold: int value representing minimum size. Default is 1.
    
    Returns:
    a list of strings containing the record id and sequence.
    """
    # create a list to store filtered records
    filtered_records = []
    
    # parse through the passed fasta's records
    for record in SeqIO.parse(fasta_path, "fasta"):
    
        # split the description of the record by semicolon
        parsed_record = record.description.split(';')
        
        # Check that the parsed record contains more than one part, get the second segment,
        # and check it starts with the string 'seqs'
        if len(parsed_record) > 1 and parsed_record[-1].startswith('seqs='):
            
            # Get the size of the sequence by the last value
            size = int(parsed_record[-1].split('=')[-1])
            
            # if the size is larger than the threshold,
            if size > threshold:
                
                # add it to the filtered records.
                filtered_records.append(">{}\n{}".format(record.id, record.seq))
    
    return filtered_records

def mmm2_cluster_aligner(mapper_path, reference_path, trimmed_reads, out_path, out_name):
    """
    mm2_cluster_aligner takes in a cluster fasta from VSEARCH as well as the 
    original trimmed data set and performs realignment on the data using 
    minimap2
    
    Parameters:
    mapper_path: path to minimap2 as str
    reference_path: path to cluster/consensus reference fasta
    trimmed_reads: path to basecalled and trimmed reads in fasta format
    out_path: path to the output directory, as str
    out_name: name the output sam file will be given, as str
    
    Returns:
    cmd: command with apppropriate minimap2 parameters and inputs
    """
    #NOTES FOR SELF: should probably have it generate the minimap2 string and not run it??? 
    #other note: since sam file is outputted, can use one of the methods above to filter it, then use the length of the sam file (assuming no headers are outputted) to get the weight of the particular cluster 
    cmd = "{} -ax map-ont --MD {} {} > {}{}.sam".format(mapper_path, reference_path, trimmed_reads, out_path, out_name) #probably doesn't need minimap2 as a separate path
    print('[Mapping]: Command Generated: "{}"'.format(cmd))
    return cmd

def weight_generation(aligned_sam_path): 
    """
    weight_generation takes in an aligned sam (primary reads only) and returns 
    a list / dictionary (CHOOSE) containing the references (clusters) from
    VSEARCH and the associated weight with that cluster (number of primary 
    aligned reads). 
    
    Parameters: 
    aligned_sam_path: filepath to inputted sam file generated from mm2_cluster_aligner
    
    Returns: 
    reference_counts: dictionary containing the different reference sequence names and the amount of reads that aligned to it
    """
    # Open the SAM file
    with pysam.AlignmentFile(sam_file_path, "r") as samfile:
        # Initialize a dictionary to hold the count of reads per reference sequence
        reference_counts = {}
        
        # Iterate over each read in the SAM file
        for read in samfile:
            if not read.is_unmapped and not read.is_secondary and not read.is_supplementary:  # Check if the read is mapped to a reference sequence
                ref_name = samfile.get_reference_name(read.reference_id)  # Get the reference sequence name
                
                if ref_name in reference_counts:
                    # If the reference sequence is already in the dictionary, increment the count
                    reference_counts[ref_name] += 1
                else:
                    # If it's the first time we see this reference sequence, initialize its count to 1
                    reference_counts[ref_name] = 1
                    
    return reference_counts #in main function, probably where weight calculation would get done or write a new function that generates the weight fasta file 

def weighted_fasta_gen(cluster_fasta, reference_counts):
    """
    weighted_fasta_gen will take a cluster fasta outputted from VSEARCH
     as well as a dictionary containing the reference 
    sequences in that sam file (clusters outputted from VSEARCH) and the number 
    of reads aligned to that particular reference sequence. It will generate 
    a fasta file with the same reference sequence except multiplied by the number 
    of reads that aligned to perfor weight cluster formation downstream. 
    
    Parameters: 
    cluster_fasta: used to extract the actual sequence of the reference 
    reference_counts: dictionary containing the names of the reference sequences (should be the same names in cluster_fasta) as well as the number of reads that aligned to that cluster after minimap2
    
    Returns:
    weighted_fasta_file_path: fasta file containing the same reference sequences as cluster_fasta except multiplied by the number of counts from reference_counts 
    """
    weighted_fasta_file_path = "weighted.fasta"
    
    with open(weighted_fasta_file_path, "w") as output_file:
        #write into the weighted fasta file
        for record in SeqIO.parse(cluster_fasta, "fasta"):
            reference_name = record.id
            # find the reference name in the cluster fasta
            
            if reference_name in reference_counts:
                count = reference_counts[reference_name]
                weighted_sequence = record.seq * count
                # multiply the reference sequence by the number of reads that aligned to it
                
                output_file.write(f">{reference_name}_weighted_{count}\n{weighted_sequence}\n")
    return weighted_fasta_file_path

def write_to_fasta(out_path, out_name, list_data):
    """
    write_to_fasta takes in an output path and file name, as well as a list
    of data to be written, and writes that data out as a fasta file at that
    path.
    
    Parameters:
    out_path: filepath to output directory, as str.
    out_name: name for the fasta file, as str.
    list_data: a list of the data to be written, formatted with each value
               being ">{id}\n{sequence}"
    
    Returns:
    the final path to the output fasta.
    """
    # create the output filename.
    out_file = out_path + out_name + ".fasta"

    if os.path.exists(out_file):
        os.remove(out_file)
    
    # open the output file in write mode.
    with open(out_file, 'w') as output_file:

        for datum in list_data:
            # write each value in the fasta to a new line. 
            output_file.write("{}\n".format(datum))
        print("[Writing Fasta]: {} lines written to {}".format(len(list_data), out_file))
        
    # return the output filepath.
    return out_file
        

def medaka_consensus_command(medaka_path, trim_fasta, filtered_fasta, out_path):
    """
    medaka_consensus_command takes in a filepath for medaka, a trimmed fasta file,
    a filtered cluster/consensus fasta, an output filepath, and then polishes/forms a consensus fasta.
    
    Parameters:
    medaka_path: path, as str, to medaka_consensus
    trim_fasta: path, as str, to trimmed fasta file
    filtered_fasta: path, as str, to filtered fasta file
    out_path: path, as str, to output directory
    """

    # Generate the command.
    cmd = "{} -i {} -d {} -o {} -m r1041_e82_400bps_hac_v4.2.0 -f -b 300".format(medaka_path, trim_fasta, filtered_fasta, out_path)
    print('[Consensus Forming]: Command Generated: "{}"'.format(cmd))
    return cmd

def extract_n_indexes(n_fasta_file):
    """
    extract_n_indexes Extracts the indexes j and k for each sequence in a barcode fasta
    file with unknown regions.
    j: the index of the first N (0 indexed)
    k: the index of the last N (0 indexed)

    Designed specifically for interplay with xf_lowqual

    Parameters:
    n_fasta_file: path to the barcode/reference fasta file

    Returns:
    the indexes, as a list containing touples???? Why:TODO
    
    """
    n_indexes = []
    for record in SeqIO.parse(n_fasta_file, "fasta"):
        sequence_str = str(record.seq)
        first_n_index = sequence_str.find('N')
        last_n_index = sequence_str.rfind('N')
        if first_n_index != -1:
            j = first_n_index
            k = last_n_index
        else:
            j = k = None
        n_indexes.append((j, k))
    return n_indexes


def rename_consensus_headers(consensus_fasta_file, j, k, output_file):
    """
    rename_consensus_headers will
    Rename the headers in the consensus FASTA file based on the provided first and last N indexes.
    All headers will be renamed using the same j and k values.

    Designed specifically for interplay with xf_lowqual

    Parameters:
    consensus_fasta_file: the consensus file output by medaka as consensus.fasta
    j: index of first unknown base
    k: index of last unknown base
    output_file: the filepath to the desired output file

    TODO:NEEDS TO BE BROUGHT UP TO SPEED WITH OTHER WRITING METHDS

    Returns:
    Path to the output file

    
    """
    with open(output_file, 'w') as outfile:
        for i, record in enumerate(SeqIO.parse(consensus_fasta_file, "fasta"), start=1):
            record.description = f"consensus {i}- BC 1: {j}, BC 2: {k}"
            record.description = f" - BC 1: {j}, BC 2: {k}"
            record.id = f"consensus_{i}"
            SeqIO.write(record, outfile, "fasta")

    return str(os.path.abspath(output_file))


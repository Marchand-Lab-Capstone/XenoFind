"""
consensus_methods.py 
J. Sumabat, N. Lai, S. Peck, Y. Huang, 3/14/2024

consensus_methods.py contains the primary methods involved in forming a first-pass
consensus on a DNA sequence with potential XNA bases. 

basecall_command() - Generates a command to run basecalling on pod5 reads using dorado
map_to_reference() - Generates a command to map a basecalled sequence to a reference fasta
vsearch_command() - Generates a command to run cluster vsearch on a fasta file
read_trim() - Generates a list of reads that are trimmed within 95% of the reference.
sort_fasta() - Generates a list of reads, sorted by length.
filter_cluster_size() - Generates a list of reads filtered to a cluster size -- NONFUNCTIONAL
write_to_fasta() - Is able to take a list of reads generated by the previous three methods, and write them to a fasta file. 
"""


from Bio import SeqIO
from Bio.Seq import Seq
import os
import pysam
import subprocess
import raw_read_merger as rrm
import setup_methods as setup
import pandas as pd
import re

def basecall_command(basecaller_path, model_type, pod5_path, out_path, out_name):
    """
    basecall_command generates a command to run the basecaller dorado.
    
    Parameters:
    basecaller_path: path to the basecaller, as str
    pod5_path: path to the pod5 file to be basecalled, as str
    out_path: path to the output directory, as str
    out_name: name the basecalled fq file will be given, as str
    
    Returns:
    a command string.
    """
    cmd = "{} basecaller {} --no-trim --emit-moves --emit-fastq {} > {}{}.fq".format(basecaller_path, model_type, pod5_path, out_path, out_name)
    print('[Basecalling]: Command Generated: "{}"'.format(cmd))
    return cmd


def map_to_reference(mapper_path, reference_path, basecall_path, out_path, out_name):
    """
    map_to_reference generates a command to use minimap2 to align the basecall with
    a reference sequence.
    
    Parameters:
    mapper_path: path to minimap2 as str
    reference_path: path to reference fasta as str
    basecall_path: path to basecalled .fq file, as str
    out_path: path to the output directory, as str
    out_name: name the output sam file will be given, as str
    """
    # Currently only supports minimap2
    cmd = "{} -ax map-ont --score-N 0 --MD --min-dp-score 10 {} {} > {}{}.sam".format(mapper_path, reference_path, basecall_path, out_path, out_name) #probably doesn't need minimap2 as a separate path

    print('[Mapping]: Command Generated: "{}"'.format(cmd))
    return cmd

def sam_to_bam(input_sam, out_path, out_name):
    """
    sam_to_bam is a function that takes in a sam file generated from minimap2 
    and generates a samtools command that can be run using os.system to 
    generated a bam file containing the sam information 
    
    Parameters: 
    input_sam: file path way to generated sam file as a string 
    out_path: file path where bam file should be generated 
    out_name: desired name of the bam file 
    """
    cmd = 'samtools view -bho {}{}.bam {}'.format(out_path, out_name, input_sam)
    bam_path = '{}{}.bam'.format(out_path, out_name)
    return cmd, bam_path
    

def filter_primary_alignments(bam_path, out_name): #can edit this function to take in an input file name 
    """
    Filters a BAM file to retain only primary alignments and outputs the result
    to a new BAM file named 'primary_alignments.bam' in the same directory as
    the input file.
    
    Parameters: 
    bam_path: path to the BAM file as a string.
    
    Returns: 
    Path to the filtered BAM file containing only primary alignments.
    """
    
    # Generating the output BAM file path
    directory = os.path.dirname(bam_path)
    output_bam = os.path.join(directory, '{}.bam'.format(out_name))
    
    # Using pysam to filter for primary alignments
    with pysam.AlignmentFile(bam_path, "rb") as infile, \
         pysam.AlignmentFile(output_bam, "wb", header=infile.header) as outfile:

        for read in infile:
            if not read.is_secondary and not read.is_supplementary and not read.is_unmapped:
                outfile.write(read)
                
    print('XenoFind [STATUS] - Primary Only BAM file generated.')

    return output_bam
        
def read_trim(bam_path):
    """
    read_trim takes in a BAM file, sorts and indexes it, then returns a list of the reads
    in that BAM file, with the query name and alignment sequence,
    so long as they are mapped and >=95% of the reference length.
    
    Parameters:
    bam_path: path to the BAM file in question as a string.
    
    Returns:
    list of queries and alignments as a string.
    """
    output_forward_list = []
    output_reverse_list = []

    # Generate paths for sorted and indexed bam
    sorted_bam_path = bam_path + ".sorted.bam"
    
    # Sorting the bam file
    sort_cmd = f'samtools sort {bam_path} -o {sorted_bam_path}'
    subprocess.run(sort_cmd, shell=True, check=True)
    
    # Indexing the sorted bam file
    index_cmd = f'samtools index {sorted_bam_path}'
    subprocess.run(index_cmd, shell=True, check=True)
    
    #Open the sorted and indexed bam using pysam
    with pysam.AlignmentFile(sorted_bam_path, 'rb') as bamfile:
        num_unmapped = 0
        num_outside = 0
        
        # for each read in the bamfile,
        for read in bamfile.fetch():
            # Increment num_unmapped if the read is unmapped
            if read.is_unmapped:
                num_unmapped += 1
                continue
                
            # Check that the read meets alignment length criteria
            if read.query_alignment_length < 0.95 * read.reference_length:
                num_outside += 1
                continue

            # Get query ID and alignment sequence
            query_id = read.query_name
            alignment_sequence = read.query_alignment_sequence

            # Determine orientation and append to the appropriate list
            if not read.is_reverse:
                # Forward read
                output_forward_list.append(f">{query_id}:fwd\n{alignment_sequence}")
            else:
                # Reverse read, reverse complement the sequence
                reverse_alignment_sequence = str(Seq(alignment_sequence).reverse_complement())
                output_reverse_list.append(f">{query_id}:rev\n{reverse_alignment_sequence}")
                
    output_list = [output_forward_list, output_reverse_list]
    
    print(f"XenoFind [STATUS] - Trimming complete: {num_unmapped} unmapped reads, {num_outside} short reads removed.")
    
    return output_list
    
    
    # Remove the sorted and indexed BAM file after processing
    os.remove(sorted_bam_path)
    # Also remove the BAM index file
    os.remove(sorted_bam_path + ".bai")
    
    return output_list
    
def write_to_fasta(out_path, out_name, list_data):
    """
    write_to_fasta takes in an output path and file name, as well as a list
    of data to be written, and writes that data out as a fasta file at that
    path.
    
    Parameters:
    out_path: filepath to output directory, as str.
    out_name: name for the fasta file, as str.
    list_data: a list of the data to be written, formatted with each value
               being ">{id}\n{sequence}"
    
    Returns:
    the final path to the output fasta.
    """
    # create the output filename.
    out_file = out_path + out_name + ".fasta"

    if os.path.exists(out_file):
        os.remove(out_file)
    
    # open the output file in write mode.
    with open(out_file, 'w') as output_file:

        for datum in list_data:
            # write each value in the fasta to a new line. 
            output_file.write("{}\n".format(datum))
        print("[Writing Fasta]: {} lines written to {}".format(len(list_data), out_file))
        
    # return the output filepath.
    return out_file


# How to validate the read_trim worked?

def sort_fasta(fasta_path):
    """
    sort_fasta takes in a fasta filepath and sorts it by length.
    
    Parameters:
    fasta_path: a path to a trimmed fasta file.

    
    Returns:
    a list of the records, sorted by length.
    """
    # get SeqRecord iterator as list by parsing the fasta file and passing fasta format.
    records = list(SeqIO.parse(fasta_path, "fasta"))
    
    # Sort the records by length using in-built sorting by sequence length.
    sorted_records = sorted(records, key=lambda x: len(x.seq))
    
    # create an empty list to hold the records to be output.
    output_records = []

    for record in sorted_records:
        output_records.append(">{}\n{}".format(record.id, record.seq))

    print("XenoFind [STATUS] - {} records sorted.".format(len(output_records)))
    
    # return the sorted_records list
    return output_records


def vsearch_command(vsearch_path, fasta_path, out_path, out_name, sim_id):
    """
    vsearch_command takes in a fasta path, a similarity value, an output_path, and an output filename,
    and generates a command to perform clustering/ rough consensus formation on the fasta file.
    
    Parameters:
    vsearch_path: path to vsearch as a string.
    fasta_path: path to the fasta file in question, as a string.
    out_path: path to output the final consensus fasta.
    out_name: filename for the fasta file. 
    sim_id: float value representing how similar the centroids can be (between 0 and 1)
    
    Returns: 
    command to perform vsearch with given directories as a string.
    """
    
    # Generate the command.
    cmd = "{} --cluster_fast {} --id {} --clusterout_sort --consout {}{}.fasta".format(vsearch_path, fasta_path, sim_id, out_path, out_name)
    cluster_path = "{}{}.fasta".format(out_path, out_name)
    print('[Vsearching]: Command Generated: "{}"'.format(cmd))
    return cmd, cluster_path

def mm2_cluster_aligner(mapper_path, reference_path, trimmed_reads, out_path, out_name):
    """
    mm2_cluster_aligner takes in a cluster fasta from VSEARCH as well as the 
    original trimmed data set and performs realignment on the data using 
    minimap2
    
    Parameters:
    mapper_path: path to minimap2 as str
    reference_path: path to cluster/consensus reference fasta
    trimmed_reads: path to basecalled and trimmed reads in fasta format
    out_path: path to the output directory, as str
    out_name: name the output sam file will be given, as str
    
    Returns:
    cmd: command with apppropriate minimap2 parameters and inputs
    """
    cmd = "{} -ax map-ont --MD {} {} > {}{}.sam".format(mapper_path, reference_path, trimmed_reads, out_path, out_name) #probably doesn't need minimap2 as a separate path
    sam_path = "{}{}.sam".format(out_path, out_name)
    
    print('[Mapping]: Command Generated: "{}"'.format(cmd))
    return cmd, sam_path

def weight_generation(aligned_bam_path):
    """
    Takes in an aligned BAM (primary reads only) and returns a dictionary containing 
    the references (clusters) from VSEARCH and the associated count of primary 
    aligned reads.
    
    Parameters: 
    aligned_bam_path: File path to inputted BAM file generated from mm2_cluster_aligner.
    
    Returns: 
    reference_counts: Dictionary containing the different reference sequence names 
    and the count of reads that aligned to each.
    """
    # Open the BAM file for reading ("rb" mode)
    with pysam.AlignmentFile(aligned_bam_path, "rb") as bamfile:
        # Initialize a dictionary to hold the count of reads per reference sequence
        reference_counts = {}
        
        # Iterate over each read in the BAM file
        for read in bamfile:
            # Check if the read is mapped and is a primary alignment (not secondary/supplementary)
            if not read.is_unmapped and not read.is_secondary and not read.is_supplementary:
                ref_name = bamfile.get_reference_name(read.reference_id)  # Get the reference sequence name
                
                if ref_name in reference_counts:
                    # If the reference sequence is already in the dictionary, increment the count
                    reference_counts[ref_name] += 1
                else:
                    # If it's the first time we see this reference sequence, initialize its count to 1
                    reference_counts[ref_name] = 1
                    
    return reference_counts

def weighted_fasta_gen(cluster_fasta, reference_counts, out_name):
    """
    weighted_fasta_gen will take a cluster fasta outputted from VSEARCH
     as well as a dictionary containing the reference 
    sequences in that sam file (clusters outputted from VSEARCH) and the number 
    of reads aligned to that particular reference sequence. It will generate 
    a fasta file with the same reference sequence except multiplied by the number 
    of reads that aligned to perfor weight cluster formation downstream. 
    
    Parameters: 
    cluster_fasta: used to extract the actual sequence of the reference 
    reference_counts: dictionary containing the names of the reference sequences (should be the same names in cluster_fasta) as well as the number of reads that aligned to that cluster after minimap2
    
    Returns:
    weighted_fasta_file_path: fasta file containing the same reference sequences as cluster_fasta except multiplied by the number of counts from reference_counts 
    """
    weighted_fasta_file_path = os.path.join(os.path.dirname(cluster_fasta), '{}.fasta'.format(out_name))
    
    with open(weighted_fasta_file_path, "w") as output_file:
        #write into the weighted fasta file
        for record in SeqIO.parse(cluster_fasta, "fasta"):
            reference_name = record.id
            sequence = str(record.seq)
            # find the reference name in the cluster fasta
            
            if reference_name in reference_counts:
                count = reference_counts[reference_name]
                i = 0
                for i in range(count):
                 # multiply the reference sequence by the number of reads that aligned to it
                    output_file.write(f">{reference_name}_weighted_{i+1}\n{sequence}\n")
                
    return weighted_fasta_file_path

def rename_consensus_headers(consensus_fasta_file, output_file):
    """
    rename_consensus_headers will
    Rename the headers in the consensus FASTA file based on the provided first and last N indexes.
    All headers will be renamed using the same j and k values.

    Designed specifically for interplay with xf_lowqual

    Parameters:
    consensus_fasta_file: the consensus file output by medaka as consensus.fasta
    output_file: the filepath to the desired output file

    Returns:
    Path to the output file
    """
    with open(output_file, 'w') as outfile:
        for i, record in enumerate(SeqIO.parse(consensus_fasta_file, "fasta"), start=1):
            header_parts = re.split(';', record.id)
            read_id_part = re.split('=', header_parts[0])
            size_part = re.split('=', header_parts[-1])
            read_id = read_id_part[-1]
            size = size_part[-1]

            new_header = f"{read_id};size={size}"
            record.id = new_header
            record.description = new_header

            SeqIO.write(record, outfile, "fasta")

    return str(os.path.abspath(output_file))
    
def cluster_size_df(cluster_fasta, dataframe): 
    print('Xemora [STATUS] - Generating cluster sizes')
    with open(cluster_fasta, "r") as fasta:
        read_id_list = [] 
        cluster_size_list = []
        for record in SeqIO.parse(fasta, "fasta"):
            header_parts = re.split(';', record.id)
            read_id_part = re.split('=', header_parts[0])
            size_part = re.split('=', header_parts[-1])
            read_id = read_id_part[-1]
            size = size_part[-1]
            read_id_list.append(read_id)
            cluster_size_list.append(size)

   # Determine the iteration number
    next_iteration = 'iteration 0' if dataframe.empty else f'iteration {len(dataframe.columns) - 1}'
    # Create a temporary dataframe from the current fasta read
    current_data = pd.DataFrame({
        'read_id': read_id_list,
        next_iteration: cluster_size_list
    })
    if dataframe.empty:
        dataframe = current_data
    else:
        # Merge existing dataframe with new data
        dataframe = dataframe.merge(current_data, on='read_id', how='outer')
    return dataframe

def filter_cluster_size(fasta_path, threshold, direction):
    """
    filter_cluster_size takes in a fasta filepath and a threshold,
    and removes all clusters that are not within that size threshold.
    
    Parameters:
    fasta_path: path to clustered/consensus fasta file, as string.
    threshold: int value representing minimum size. Default is 1.
    
    Returns:
    a file pathway with the final filtered fasta
    """
    # Create a list to store filtered records
    filtered_records = []
    
    # Parse through the passed fasta's records
    for record in SeqIO.parse(fasta_path, "fasta"):
        # Split the description of the record by semicolon
        parsed_record = record.description.split(';')
        
        # Check that the parsed record contains more than one part, get the second segment,
        # and check it starts with the string 'size'
        if parsed_record[-1].startswith('size='):
            # Get the size of the sequence by the last value
            size = int(parsed_record[-1].split('=')[-1])
            
            # If the size is larger than the threshold,
            if size > threshold:
                # Add it to the filtered records.
                filtered_records.append(">{}\n{}".format(record.id, str(record.seq)))
    
    # Generate the output file path using string formatting to include the direction
    output_path = os.path.join(os.path.dirname(fasta_path), f'filtered_consensus_{direction}.fasta')
    
    # Write the final fasta file
    with open(output_path, 'w') as output_file:
        for record in filtered_records:
            output_file.write(record + "\n")
    
    # Return the path of the generated file
    return output_path

def merge_fasta_files(files_to_merge, output_file):
    """
    Merges multiple FASTA files into one.

    Parameters:
    files_to_merge: list of str, the full paths to the FASTA files to merge.
    output_filename: str, the name of the final merged FASTA file.

    Returns:
    Path to the merged FASTA file.
    """
    if not files_to_merge:
        return None

    records = []

    # Iterate over each file and append its records to the list
    for file_path in files_to_merge:
        records.extend(list(SeqIO.parse(file_path, "fasta")))


    # Writing combined records to the new file
    with open(output_file, 'w') as output_file:
        SeqIO.write(records, output_file, "fasta")

    return output_file
